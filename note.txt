- Medium - Prepare cdc oracle
https://medium.com/plumbersofdatascience/unlocking-real-time-data-synchronisation-oracle-to-kafka-using-change-data-capture-cdc-fd4687e0c877

- Github 
https://github.com/sami12rom/kafka-connect-oracle-cdc/blob/main/docker-compose.yml

- Oracle 
https://stackoverflow.com/questions/73567042/starting-an-oracle-db-using-docker-compose

- Cara Install oracle 
https://github.com/steveswinsburg/oracle19c-docker

- Sink To HDFS3
https://docs.confluent.io/kafka-connectors/hdfs3-sink/current/overview.html#hdfs3-sink-exactly-once-delivery

https://medium.com/@annick2908.au/using-kafka-connect-for-regular-loading-of-partitioned-data-into-hadoop-2f03c530713d

- Connect cdc oracle (Confluent)
curl -s -X POST -H 'Content-Type: application/json' --data @oracle_cdc_connector.json http://localhost:8083/connectors | jq

- Create CDC oracle (Debezium) 
curl -s -X POST -H 'Content-Type: application/json' --data @connectors_script/debezium_oracle_cdc_connector.json http://localhost:8083/connectors | jq


- Sink to HDFS (Confluent)
curl -s -X POST -H 'Content-Type: application/json' --data @connectors_script/hdfs_hive_sink_connector.json http://localhost:8083/connectors | jq

- Delete Connector
curl -s -X DELETE http://localhost:8083/connectors/oracle-01
curl http://localhost:8083/connectors

- Step Running in docker compose :
1. Create network
$ docker network create kafka-hadoop

2. Create volumes
$ docker volume create oracledb-19c

3. install environment kafka 
$ docker compose up -d 

4. install environment hadoop 
docker compose -f docker-compose-port-dev.yml
